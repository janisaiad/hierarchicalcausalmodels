{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94d4313",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "e3408d19",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "class CausalGenerator:\n",
    "    def __init__(self, graph, data,unit_vars,subunit_vars,sizes):\n",
    "        self.graph = nx.DiGraph(graph)\n",
    "        self.data = data\n",
    "        self.processed_data = self.preprocess_data(unit_vars,subunit_vars,sizes)\n",
    "        \n",
    "    def preprocess_data(self,unit_vars,subunit_vars,sizes):\n",
    "        # Aggregate subunit nodes\n",
    "        data = {}\n",
    "        \n",
    "        for var in unit_vars:\n",
    "            for i in range(len(sizes)):\n",
    "                data[var+str(i)] = self.data[var+str(i)]\n",
    "        for var in subunit_vars:\n",
    "            for i in range(len(sizes)):\n",
    "                s=0\n",
    "                for j in range(sizes[i]):\n",
    "                    s+=self.data['_'+var+str(i)+'_'+str(j)]\n",
    "            data[var] = s/sizes[i]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def generate(self, noise_dist, transition_funcs, sizes,special_node,intervention_value):\n",
    "        generated = {}\n",
    "        for node in nx.topological_sort(self.graph):\n",
    "            if node is special_node:\n",
    "                generated[node] = intervention_value\n",
    "            else:\n",
    "                for i in range(len(sizes)):\n",
    "                    x=np.random.uniform()\n",
    "                    parents = list(self.graph.predecessors(node))\n",
    "                    if not parents:\n",
    "                        generated[node+str(i)] = noise_dist[node](x)\n",
    "                    else:\n",
    "                        parent_values = [generated[p+str(i)] for p in parents]\n",
    "                        generated[node+str(i)] = transition_funcs[node](*parent_values) + noise_dist[node](x)\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        return generated\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6cd4931186d1cf2",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def load_data_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a JSON file and return it as a dictionary.\n",
    "    \n",
    "    :param file_path: Path to the JSON file\n",
    "    :return: Dictionary containing the loaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        print(f\"Data successfully loaded from {file_path}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f551289d00ebce7f",
   "metadata": {},
   "source": [
    "\n",
    "# Example usage:\n",
    "graph = [('a', '_b'), ('a', 'c'), ('_b', 'c'), ('c', '_d'), ('_b', '_d'), ('_d', 'e'), ('c', 'e')]\n",
    "data = load_data_from_json('data/sampled_data.json')\n",
    "\n",
    "unit_vars = ['a', 'c', 'e']\n",
    "subunit_vars = ['d', 'b']\n",
    "sizes = [50]*50\n",
    "\n",
    "generator = CausalGenerator(graph, data,unit_vars,subunit_vars,sizes)\n",
    "noise_dist = {\n",
    "    'a': lambda x: norm.ppf(x),\n",
    "    '_b': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "    'c': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "    '_d': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "    'e': lambda x: norm.ppf(x, loc=0, scale=1)\n",
    "}\n",
    "\n",
    "transition_funcs = {\n",
    "    '_b': lambda a: a,\n",
    "    'c': lambda a, b: a**3 + (b+1)**2,\n",
    "    '_d': lambda b, c: b+1 - c,\n",
    "    'e': lambda c, d: c + d+1\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e025a5278ee0ced",
   "metadata": {},
   "source": [
    "\n",
    "generated_data = generator.generate(noise_dist, transition_funcs,sizes,special_node='b',intervention_value=0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ee9e21be45a450f",
   "metadata": {},
   "source": [
    "print(generated_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "91f743bb",
   "metadata": {},
   "source": [
    "# Generate arrays for each variable\n",
    "a_array = [generated_data[f'a{k}'] for k in range(len(sizes))]\n",
    "b_array = [generated_data[f'_b{k}'] for k in range(len(sizes))]\n",
    "c_array = [generated_data[f'c{k}'] for k in range(len(sizes))]\n",
    "d_array = [generated_data[f'_d{k}'] for k in range(len(sizes))]\n",
    "e_array = [generated_data[f'e{k}'] for k in range(len(sizes))]\n",
    "\n",
    "# Print the arrays\n",
    "print(\"a arrays:\", a_array)\n",
    "print(\"b arrays:\", b_array)\n",
    "print(\"c arrays:\", c_array)\n",
    "print(\"d arrays:\", d_array)\n",
    "print(\"e arrays:\", e_array)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f86562d9",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.integrate import quad"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9afbd272",
   "metadata": {},
   "source": [
    "\n",
    "def kl_divergence(p, q, bandwidth='scott'):\n",
    "    \"\"\"\n",
    "    Calcule la divergence KL entre deux distributions empiriques représentées par des tableaux,\n",
    "    en utilisant l'estimation de densité par noyau.\n",
    "    \n",
    "    :param p: Premier tableau de données\n",
    "    :param q: Second tableau de données\n",
    "    :param bandwidth: Méthode pour estimer la largeur de bande ('scott', 'silverman' ou un nombre)\n",
    "    :return: Valeur de la divergence KL\n",
    "    \"\"\"\n",
    "    # Assurez-vous que les tableaux ont la même taille\n",
    "    min_len = min(len(p), len(q))\n",
    "    p = p[:min_len]\n",
    "    q = q[:min_len]\n",
    "    \n",
    "    # Estimation de densité par noyau\n",
    "    kde_p = gaussian_kde(p, bw_method=bandwidth)\n",
    "    kde_q = gaussian_kde(q, bw_method=bandwidth)\n",
    "    \n",
    "    # Créez un espace d'échantillonnage\n",
    "    x = np.linspace(min(np.min(p), np.min(q)), max(np.max(p), np.max(q)), 10000)\n",
    "    \n",
    "    # Estimez les densités\n",
    "    p_density = kde_p(x)\n",
    "    q_density = kde_q(x)\n",
    "    \n",
    "    # Ajoutez un petit epsilon pour éviter la division par zéro\n",
    "    epsilon = 1e-10\n",
    "    p_density += epsilon\n",
    "    q_density += epsilon\n",
    "    \n",
    "    # Normalisez les densités\n",
    "    p_density /= np.sum(p_density)\n",
    "    q_density /= np.sum(q_density)\n",
    "    \n",
    "    # Calculez la divergence KL\n",
    "    return np.sum(p_density * np.log(p_density / q_density))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c34e9ece",
   "metadata": {},
   "source": [
    "\n",
    "# Select two arrays for comparison\n",
    "array1 = e_array  # First array from e_arrays\n",
    "array2 = [data[f'e{i}'] for i in range(50)]  # Second array from the original data\n",
    "\n",
    "arrayA = [data[f'a{i}'] for i in range(50)]  # Second array from the original data\n",
    "arrayB = np.array([np.mean(np.array([data[f'_b{i}_{j}'] for j in range(50)])) for i in range(50)])  # Second array from the original data\n",
    "arrayC = [data[f'c{i}'] for i in range(50)]  # Second array from the original data\n",
    "arrayD = np.array([np.mean(np.array([data[f'_d{i}_{j}'] for j in range(50)])) for i in range(50)])  # Second array from the original data\n",
    "arrayE = [data[f'e{i}'] for i in range(50)]  # Second array from the original data\n",
    "\n",
    "arrayB_full = np.array([[data[f'_b{i}_{j}'] for i in range(50)] for j in range(50)])  # Second array from the original data\n",
    "arrayD_full = np.array([[data[f'_d{i}_{j}'] for i in range(50)] for j in range(50)])  # Second array from the original data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe1ab5d5",
   "metadata": {},
   "source": [
    "\n",
    "# Compute KL divergence\n",
    "kl_div = kl_divergence(array1, array2)\n",
    "\n",
    "# Print the KL divergence\n",
    "print(f\"KL divergence between the two arrays E: {kl_div}\")\n",
    "# high kl div okay\n",
    " \n",
    "kl_div = kl_divergence(arrayA, a_array)\n",
    "print(f\"KL divergence between the two arrays A: {kl_div}\")\n",
    "\n",
    "kl_div = kl_divergence(arrayB, b_array)\n",
    "print(f\"KL divergence between the two arrays B: {kl_div}\")\n",
    "\n",
    "kl_div = kl_divergence(arrayC, c_array)\n",
    "print(f\"KL divergence between the two arrays C: {kl_div}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b6599e27",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_arrays_with_kde(original, resampled, title):\n",
    "    plt.close('all')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot original data\n",
    "    sns.kdeplot(original, shade=True, color=\"blue\", label=\"Original\")\n",
    "    plt.scatter(original, [0]*len(original), color=\"blue\", alpha=0.5)\n",
    "    \n",
    "    # Plot resampled data\n",
    "    sns.kdeplot(resampled, shade=True, color=\"red\", label=\"Resampled\")\n",
    "    plt.scatter(resampled, [0]*len(resampled), color=\"red\", alpha=0.5)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for E\n",
    "plot_arrays_with_kde(array2, array1, \"Distribution of E: Original vs Resampled\")\n",
    "\n",
    "# Plot for A\n",
    "plot_arrays_with_kde(arrayA, a_array, \"Distribution of A: Original vs Resampled\")\n",
    "\n",
    "# Plot for B\n",
    "plot_arrays_with_kde(arrayB, b_array, \"Distribution of B: Original vs Resampled\")\n",
    "\n",
    "# Plot for C\n",
    "plot_arrays_with_kde(arrayC, c_array, \"Distribution of C: Original vs Resampled\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69707546",
   "metadata": {},
   "source": [
    "def experiment(n):\n",
    "    kldivs= []\n",
    "    for k in range(n):\n",
    "        # Example usage:\n",
    "        graph = [('a', '_b'), ('a', 'c'), ('_b', 'c'), ('c', '_d'), ('_b', '_d'), ('_d', 'e'), ('c', 'e')]\n",
    "        data = load_data_from_json(f'data/sampled_data_{k}.json')\n",
    "        print(str(k) + \" is in progress\")\n",
    "        \n",
    "        unit_vars = ['a', 'c', 'e']\n",
    "        subunit_vars = ['d', 'b']\n",
    "        sizes = [50]*50\n",
    "        \n",
    "        generator = CausalGenerator(graph, data,unit_vars,subunit_vars,sizes)\n",
    "        noise_dist = {\n",
    "            'a': lambda x: norm.ppf(x),\n",
    "            '_b': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "            'c': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "            '_d': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "            'e': lambda x: norm.ppf(x, loc=0, scale=1)\n",
    "        }\n",
    "        \n",
    "        transition_funcs = {\n",
    "            '_b': lambda a: (a+1)**2,\n",
    "            'c': lambda a, b: a**3 + (b+1)**2,\n",
    "            '_d': lambda b, c: (b+1)**2 - c**3,\n",
    "            'e': lambda c, d: c + (d+1)**2\n",
    "        }\n",
    "        \n",
    "            \n",
    "        generated_data = generator.generate(noise_dist, transition_funcs,sizes,special_node='b',intervention_value=0)\n",
    "            # Generate arrays for each variable\n",
    "        a_array = [generated_data[f'a{k}'] for k in range(len(sizes))]\n",
    "        b_array = [generated_data[f'_b{k}'] for k in range(len(sizes))]\n",
    "        c_array = [generated_data[f'c{k}'] for k in range(len(sizes))]\n",
    "        d_array = [generated_data[f'_d{k}'] for k in range(len(sizes))]\n",
    "        e_array = [generated_data[f'e{k}'] for k in range(len(sizes))]\n",
    "            \n",
    "            \n",
    "        # Select two arrays for comparison\n",
    "        array1 = e_array  # First array from e_arrays\n",
    "        array2 = [data[f'e{i}'] for i in range(50)]  # Second array from the original data\n",
    "        \n",
    "        arrayA = [data[f'a{i}'] for i in range(50)]  # Second array from the original data\n",
    "        arrayB = [np.mean(np.array([data[f'_b{i}_{j}'] for j in range(50)])) for i in range(50)]  # Second array from the original data\n",
    "        arrayC = [data[f'c{i}'] for i in range(50)]  # Second array from the original data\n",
    "        \n",
    "        arrayD = [np.mean(np.array([data[f'_d{i}_{j}'] for j in range(50)])) for i in range(50)]  # Second array from the original data\n",
    "        \n",
    "        kldiv = {'c': kl_divergence(arrayC, c_array), 'b': kl_divergence(arrayB, b_array), 'a': kl_divergence(arrayA, a_array), 'd': kl_divergence(arrayD, d_array), 'e': kl_divergence(array2, array1)}\n",
    "        \n",
    "        \n",
    "        kldivs.append(kldiv)\n",
    "        \n",
    "    return kldivs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65a6c5e30018efd5",
   "metadata": {},
   "source": [
    "kldivs = experiment(100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d7b8d4a962e9943",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Prepare data for plotting\n",
    "variables = list(kldivs[0].keys())  # Assuming kldivs is a list of dictionaries\n",
    "\n",
    "# Create subplots for each variable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    # Extract KL divergence values for the current variable\n",
    "    kl_values = [kldiv[var] for kldiv in kldivs]\n",
    "    \n",
    "    # Create a KDE plot for the current variable\n",
    "    sns.kdeplot(kl_values, ax=axes[i], shade=True, color='skyblue')\n",
    "    \n",
    "    # Customize the subplot\n",
    "    axes[i].set_title(f'KL Divergence Distribution for Variable {var}')\n",
    "    axes[i].set_xlabel('KL Divergence')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "# Remove the empty subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24482d78d36c10e1",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate mean and standard deviation for each variable\n",
    "for var in variables:\n",
    "    kl_values = [kldiv[var] for kldiv in kldivs]\n",
    "    mean = np.mean(kl_values)\n",
    "    std = np.std(kl_values)\n",
    "    print(f\"Variable {var}:\")\n",
    "    print(f\"  Mean KL Divergence: {mean:.4f}\")\n",
    "    print(f\"  Standard Deviation: {std:.4f}\")\n",
    "    print()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "987f567cd5b5c634",
   "metadata": {},
   "source": [
    "print(arrayB_full)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8314f4dba4f1ffe5",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming we have arrays arrayB_full, arrayD_full, b_array, and d_array\n",
    "# arrayB_full and arrayD_full are the true values, b_array and d_array are the predicted values\n",
    "\n",
    "# Colors for true and predicted values\n",
    "true_color = 'blue'\n",
    "pred_color = 'red'\n",
    "\n",
    "for i in range(50):\n",
    "    # Create a new figure for each unit\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    # Plot true b and d values\n",
    "    plt.scatter(arrayB_full[i,:], arrayD_full[i,:], color=true_color, label='True', marker='o')\n",
    "    \n",
    "    # Plot predicted b and d values\n",
    "    # Check if the predicted values are scalars or arrays\n",
    "    if np.isscalar(b_array[i]) and np.isscalar(d_array[i]):\n",
    "        plt.scatter(b_array[i], d_array[i], color='black', label='Predicted', marker='x')\n",
    "    else:\n",
    "        plt.scatter(b_array[i], d_array[i], color='black', label='Predicted', marker='x')\n",
    "    \n",
    "    # Set title and labels for the plot\n",
    "    plt.title(f'Unit {i}')\n",
    "    plt.xlabel('B')\n",
    "    plt.ylabel('D')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Adjust the layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Explanation of the code:\n",
    "print(\"This code generates scatter plots for 50 units, comparing true and predicted values of B and D.\")\n",
    "print(\"Each plot represents a single unit, with blue circles (o) for true values and black crosses (x) for predicted values.\")\n",
    "print(\"The true values are taken from arrayB_full and arrayD_full, while predicted values come from b_array and d_array.\")\n",
    "print(\"This visualization helps in understanding how well the predictions match the true values for each unit.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40c11ed1",
   "metadata": {},
   "source": [
    "AUC ROC"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "765176c804ea1cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4914274b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "from scipy.special import kl_div\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62b02ecf9edac1c7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def experiment(n):\n",
    "    kldivs = []\n",
    "    roc_auc_scores = []\n",
    "    \n",
    "    for k in range(n):\n",
    "        print(f\"{k} is in progress\")\n",
    "        \n",
    "        # Load data and set up the experiment\n",
    "        graph = [('a', '_b'), ('a', 'c'), ('_b', 'c'), ('c', '_d'), ('_b', '_d'), ('_d', 'e'), ('c', 'e')]\n",
    "        data = load_data_from_json(f'data/sampled_data_{k}.json')\n",
    "        \n",
    "        unit_vars = ['a', 'c', 'e']\n",
    "        subunit_vars = ['d', 'b']\n",
    "        sizes = [50]*50\n",
    "        \n",
    "        generator = CausalGenerator(graph, data, unit_vars, subunit_vars, sizes)\n",
    "        \n",
    "        noise_dist = {\n",
    "            'a': lambda x: norm.ppf(x),\n",
    "            '_b': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "            'c': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "            '_d': lambda x: norm.ppf(x, loc=0, scale=1),\n",
    "            'e': lambda x: norm.ppf(x, loc=0, scale=1)\n",
    "        }\n",
    "        \n",
    "        transition_funcs = {\n",
    "            '_b': lambda a: (a+1)**2,\n",
    "            'c': lambda a, b: a**3 + (b+1)**2,\n",
    "            '_d': lambda b, c: (b+1)**2 - c**3,\n",
    "            'e': lambda c, d: c + (d+1)**2\n",
    "        }\n",
    "        \n",
    "        generated_data = generator.generate(noise_dist, transition_funcs, sizes, special_node='b', intervention_value=0)\n",
    "        \n",
    "        # Generate arrays for each variable\n",
    "        a_array = np.array([generated_data[f'a{i}'] for i in range(len(sizes))])\n",
    "        b_array = np.array([generated_data[f'_b{i}'] for i in range(len(sizes))])\n",
    "        c_array = np.array([generated_data[f'c{i}'] for i in range(len(sizes))])\n",
    "        d_array = np.array([generated_data[f'_d{i}'] for i in range(len(sizes))])\n",
    "        e_array = np.array([generated_data[f'e{i}'] for i in range(len(sizes))])\n",
    "        \n",
    "        # Get arrays from original data\n",
    "        arrayA = np.array([data[f'a{i}'] for i in range(50)])\n",
    "        arrayB = np.array([np.mean([data[f'_b{i}_{j}'] for j in range(50)]) for i in range(50)])\n",
    "        arrayC = np.array([data[f'c{i}'] for i in range(50)])\n",
    "        arrayD = np.array([np.mean([data[f'_d{i}_{j}'] for j in range(50)]) for i in range(50)])\n",
    "        arrayE = np.array([data[f'e{i}'] for i in range(50)])\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        kldiv = {\n",
    "            'a': kl_divergence(arrayA, a_array),\n",
    "            'b': kl_divergence(arrayB, b_array),\n",
    "            'c': kl_divergence(arrayC, c_array),\n",
    "            'd': kl_divergence(arrayD, d_array),\n",
    "            'e': kl_divergence(arrayE, e_array)\n",
    "        }\n",
    "        kldivs.append(kldiv)\n",
    "        \n",
    "        # Prepare data for KNN classification\n",
    "        X = np.concatenate([a_array, b_array, c_array, d_array, e_array, \n",
    "                            arrayA, arrayB, arrayC, arrayD, arrayE])\n",
    "        y = np.concatenate([np.ones(len(a_array) * 5), np.zeros(len(arrayA) * 5)])\n",
    "        \n",
    "        # Split data for training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Train KNN classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X_train.reshape(-1, 1), y_train)\n",
    "        \n",
    "        # Predict and calculate ROC AUC score\n",
    "        y_pred = knn.predict_proba(X_test.reshape(-1, 1))[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        \n",
    "        print(f\"ROC AUC Score for iteration {k}: {roc_auc}\")\n",
    "    \n",
    "    # Print average ROC AUC score\n",
    "    avg_roc_auc = np.mean(roc_auc_scores)\n",
    "    print(f\"Average ROC AUC Score: {avg_roc_auc}\")\n",
    "    \n",
    "    return kldivs, roc_auc_scores\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "440059790d9094eb",
   "metadata": {},
   "source": [
    "\n",
    "# Run the experiment\n",
    "n_iterations = 100  # You can adjust this number\n",
    "kldivs, roc_auc_scores = experiment(n_iterations)\n",
    "\n",
    "# Plot ROC AUC scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(n_iterations), roc_auc_scores, marker='o')\n",
    "plt.title('ROC AUC Scores across Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot KL divergences\n",
    "plt.figure(figsize=(12, 6))\n",
    "for var in ['a', 'b', 'c', 'd', 'e']:\n",
    "    kl_values = [kldiv[var] for kldiv in kldivs]\n",
    "    plt.plot(range(n_iterations), kl_values, marker='o', label=f'Variable {var}')\n",
    "plt.title('KL Divergences across Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b8864ba69659dd0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
