{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T18:52:28.031121Z",
     "start_time": "2024-09-07T18:52:28.024670Z"
    }
   },
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.integrate import quad\n",
    "import pymc as pm\n"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T18:52:28.686874Z",
     "start_time": "2024-09-07T18:52:28.678275Z"
    }
   },
   "source": [
    "def preprocess_data(self):\n",
    "        data = {}\n",
    "        for var in self.unit_vars:\n",
    "            for i in range(len(self.sizes)):\n",
    "                data[var+str(i)] = self.data[var+str(i)]\n",
    "        for var in self.subunit_vars:\n",
    "            for i in range(len(self.sizes)):\n",
    "                s = 0\n",
    "                for j in range(self.sizes[i]):\n",
    "                    s += self.data['_'+var+str(i)+'_'+str(j)]\n",
    "                data[var+str(i)] = s / self.sizes[i]\n",
    "        return data"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T18:52:29.384861Z",
     "start_time": "2024-09-07T18:52:29.359682Z"
    }
   },
   "source": [
    "\n",
    "class HierarchicalBayesSampler:\n",
    "    def __init__(self, graph, data, unit_vars, subunit_vars, sizes):\n",
    "        self.graph = nx.DiGraph(graph)\n",
    "        self.data = data\n",
    "        self.unit_vars = unit_vars\n",
    "        self.subunit_vars = subunit_vars\n",
    "        self.sizes = sizes\n",
    "        self.processed_data = self.preprocess_data()\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        data = {}\n",
    "        for var in self.unit_vars:\n",
    "            for i in range(len(self.sizes)):\n",
    "                data[var+str(i)] = self.data[var+str(i)]\n",
    "        for var in self.subunit_vars:\n",
    "            for i in range(len(self.sizes)):\n",
    "                s = 0\n",
    "                for j in range(self.sizes[i]):\n",
    "                    s += self.data['_'+var+str(i)+'_'+str(j)]\n",
    "                data[var+str(i)] = s / self.sizes[i]\n",
    "        return data\n",
    "\n",
    "    def generate(self):  # num_samples est maintenant 2\n",
    "        num_samples = self.sizes[0]\n",
    "        with pm.Model() as model:\n",
    "            # Hyperpriors\n",
    "            mu = {var: pm.Normal(f'mu_{var}', mu=0, sigma=1) for var in self.unit_vars + self.subunit_vars}\n",
    "            sigma = {var: pm.HalfNormal(f'sigma_{var}', sigma=1) for var in self.unit_vars + self.subunit_vars}\n",
    "\n",
    "            # Priors pour les variables de niveau supérieur\n",
    "            variables = {}\n",
    "            for var in self.unit_vars:\n",
    "                variables[var] = pm.Normal(var, mu=mu[var], sigma=sigma[var], shape=len(self.sizes))\n",
    "\n",
    "            # Priors pour les variables de niveau inférieur (sous-unités)\n",
    "            subunit_variables = {}\n",
    "            for var in self.subunit_vars:\n",
    "                for i in range(len(self.sizes)):\n",
    "                    subunit_variables[f'{var}{i}'] = pm.Normal(f'{var}{i}', mu=mu[var], sigma=sigma[var], shape=self.sizes[i])\n",
    "\n",
    "            # Likelihood pour les variables de niveau supérieur\n",
    "            for var in self.unit_vars:\n",
    "                pm.Normal(f'obs_{var}', mu=variables[var], sigma=1, \n",
    "                        observed=np.array([self.processed_data[f'{var}{i}'] for i in range(len(self.sizes))]))\n",
    "\n",
    "            # Likelihood pour les variables de niveau inférieur (sous-unités)\n",
    "            for var in self.subunit_vars:\n",
    "                for i in range(len(self.sizes)):\n",
    "                    pm.Normal(f'obs_{var}{i}', \n",
    "                            mu=subunit_variables[f'{var}{i}'], \n",
    "                            sigma=1, \n",
    "                            observed=np.array([self.data[f'_{var}{i}_{j}'] for j in range(self.sizes[i])]))\n",
    "\n",
    "            # Sampling\n",
    "            trace = pm.sample(num_samples, return_inferencedata=False)\n",
    "\n",
    "        # Extraction des échantillons\n",
    "        generated_data = {}\n",
    "        for var in self.unit_vars:\n",
    "            generated_data[var] = trace[var][0]  # Prend le premier (et seul) échantillon\n",
    "\n",
    "        # Extraction et génération des sous-unités\n",
    "        for var in self.subunit_vars:\n",
    "            for i in range(len(self.sizes)):\n",
    "                generated_data[f'{var}{i}'] = trace[f'{var}{i}'][0]  # Prend le premier (et seul) échantillon\n",
    "\n",
    "        return generated_data\n",
    "    \n",
    "    def generate_cond(self):\n",
    "        num_samples = self.sizes[0]\n",
    "        with pm.Model() as model:\n",
    "            # Hyperpriors pour les autres variables\n",
    "            mu = {var: pm.Normal(f'mu_{var}', mu=0, sigma=1) for var in self.unit_vars + ['d']}\n",
    "            sigma = {var: pm.HalfNormal(f'sigma_{var}', sigma=1) for var in self.unit_vars + ['d']}\n",
    "\n",
    "            # Priors pour les variables de niveau supérieur\n",
    "            variables = {}\n",
    "            for var in self.unit_vars:\n",
    "                variables[var] = pm.Normal(var, mu=mu[var], sigma=sigma[var], shape=len(self.sizes))\n",
    "\n",
    "            # Prior fixe pour b (loi normale standard)\n",
    "            b_fixed = pm.Normal('b_fixed', mu=0, sigma=1, shape=(len(self.sizes), max(self.sizes)))\n",
    "\n",
    "            # Priors pour les autres variables de niveau inférieur (d)\n",
    "            subunit_variables = {}\n",
    "            for i in range(len(self.sizes)):\n",
    "                subunit_variables[f'd{i}'] = pm.Normal(f'd{i}', mu=mu['d'], sigma=sigma['d'], shape=self.sizes[i])\n",
    "\n",
    "            # Likelihood pour les variables de niveau supérieur\n",
    "            for var in self.unit_vars:\n",
    "                pm.Normal(f'obs_{var}', mu=variables[var], sigma=1, \n",
    "                        observed=np.array([self.processed_data[f'{var}{i}'] for i in range(len(self.sizes))]))\n",
    "\n",
    "            # Likelihood pour b (utilisant la loi fixe)\n",
    "            for i in range(len(self.sizes)):\n",
    "                pm.Normal(f'obs_b{i}', mu=0, sigma=1,  # Loi normale standard\n",
    "                        observed=np.array([self.data[f'_b{i}_{j}'] for j in range(self.sizes[i])]))\n",
    "\n",
    "            # Likelihood pour d\n",
    "            for i in range(len(self.sizes)):\n",
    "                pm.Normal(f'obs_d{i}', \n",
    "                        mu=subunit_variables[f'd{i}'], \n",
    "                        sigma=1, \n",
    "                        observed=np.array([self.data[f'_d{i}_{j}'] for j in range(self.sizes[i])]))\n",
    "\n",
    "            # Sampling\n",
    "            trace = pm.sample(num_samples, return_inferencedata=False)\n",
    "\n",
    "        # Extraction des échantillons\n",
    "        generated_data = {}\n",
    "        for var in self.unit_vars:\n",
    "            generated_data[var] = trace[var][0]\n",
    "\n",
    "        # Génération de b (toujours à partir d'une loi normale standard)\n",
    "        for i in range(len(self.sizes)):\n",
    "            generated_data[f'b{i}'] = np.random.normal(0, 1, size=self.sizes[i])\n",
    "\n",
    "        # Extraction de d\n",
    "        for i in range(len(self.sizes)):\n",
    "            generated_data[f'd{i}'] = trace[f'd{i}'][0]\n",
    "\n",
    "        return generated_data\n",
    "\n",
    "\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    def integrand(x):\n",
    "        px = p(x)\n",
    "        qx = q(x)\n",
    "        epsilon = 1e-10\n",
    "        return np.where((px > 0) & (qx > 0),\n",
    "                        px * (np.log(px + epsilon) - np.log(qx + epsilon)),\n",
    "                        0)\n",
    "    \n",
    "    result, _ = quad(integrand, -np.inf, np.inf, limit=1000)\n",
    "    return result\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T18:52:29.404001Z",
     "start_time": "2024-09-07T18:52:29.386459Z"
    }
   },
   "source": [
    "n_schools = 10\n",
    "n_students = 50\n",
    "# Example usage\n",
    "graph = [('a', '_b'), ('a', 'c'), ('_b', 'c'), ('c', '_d'), ('_b', '_d'), ('_d', 'e'), ('c', 'e')]\n",
    "data = {\n",
    "    **{f'a{i}': i + 1 for i in range(n_schools)},\n",
    "    **{f'c{i}': i + 5 for i in range(n_schools)},\n",
    "    **{f'e{i}': i + 11 for i in range(n_schools)}\n",
    "}\n",
    "\n",
    "# Generate data for b and d\n",
    "for i in range(n_schools):\n",
    "    for j in range(n_students):\n",
    "        data[f'_b{i}_{j}'] = 2*i + j % 2 + 1  # This creates a slight variation between students\n",
    "        data[f'_d{i}_{j}'] = 2*i + j % 2 + 7  # This creates a slight variation between students\n",
    "\n",
    "unit_vars = ['a', 'c', 'e']\n",
    "subunit_vars = ['b', 'd']\n",
    "sizes = [n_students] * n_schools\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sampling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-07T18:52:30.190231Z"
    }
   },
   "source": [
    "\n",
    "sampler = HierarchicalBayesSampler(graph, data, unit_vars, subunit_vars, sizes)\n",
    "generated_data = sampler.generate()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 50 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [mu_a, mu_c, mu_e, mu_b, mu_d, sigma_a, sigma_c, sigma_e, sigma_b, sigma_d, a, c, e, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, d0, d1, d2, d3, d4, d5, d6, d7, d8, d9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34cb7a6db7084e73bb881a4c35dd2ba5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "print( generated_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "generated_data_cond = sampler.generate_cond()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "print(generated_data_cond)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Compute KL divergence for 'e' variable\n",
    "e_generated = generated_data['e']\n",
    "e_original = np.array([sampler.processed_data[f'e{i}'] for i in range(len(sizes))])\n",
    "\n",
    "e_generated_cond = generated_data_cond['e']\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "kde_generated_cond = gaussian_kde(e_generated_cond)\n",
    "kde_generated = gaussian_kde(e_generated)\n",
    "kde_original = gaussian_kde(e_original)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T18:45:37.366266Z",
     "start_time": "2024-09-07T18:45:37.159638Z"
    }
   },
   "source": [
    "kl_div = kl_divergence(kde_original, kde_generated)\n",
    "print(f\"KL divergence between original 'e' and generated 'e': {kl_div}\")\n",
    "kl_div_cond = kl_divergence(kde_original, kde_generated_cond)\n",
    "print(f\"KL divergence between original 'e' and generated 'e' with conditionning: {kl_div_cond}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence between original 'e' and generated 'e': 0.047556381868144024\n",
      "KL divergence between original 'e' and generated 'e' with conditionning: 0.011056412213653687\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
